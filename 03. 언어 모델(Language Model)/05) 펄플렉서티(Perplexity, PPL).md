테스트 데이터에 대해서 빠르게 식으로 계산된느 간단한 평가 방법  
→ 펄플렉서티(PPL) : 모델 내에서 자신의 성능을 수치화하여 결과를 반환

# 1. 언어 모델의 평가 방법(Evaluation metrics) : PPL
PPL : 언어 모델을 평가하기 위한 평가 지표, '헷갈리는 정도' : '낮을수록' 고성능의 언어 모델  

→ 문장의 길이로 정규화된 문장 확률의 역수 

![image](https://user-images.githubusercontent.com/57162812/149069464-422a37f2-2297-430e-adbb-600eee9367ee.png)  

→ (체인룰 적용) 

![image](https://user-images.githubusercontent.com/57162812/149069575-32c526ab-a491-4c93-8ea1-906e2364a766.png)  

→ (bigrma 적용) 

![image](https://user-images.githubusercontent.com/57162812/149069653-42ff7b61-2e20-45ba-8866-777b16e1f329.png)

# 2. 분기 계수(Branching factor)
PPL : 언어 모델이 특정 시점에서 평균적으로 몇 개의 선택지를 가지고 고민하고 있는지

(가정) 언어 모델에 어떤 테스트 데이터를 주고 측정했더니 PPL이 10이 나왔다.  
→ 해당 언어 모델은 테스트 데이터에 대해서 다음 단어를 예측하는 모든 시점마다 평균 10개의 단어를 기지고 어떤 것이 정답인지 고민하고 있다.  
→ PPL이 더 낮은 언어 모델의 성능이 더 좋다.

평가 방법에 있어서 주의할 점  
- PPL의 값이 낮다 = 테스트 데이터 상에서 높은 정확를 보인다.
- PPL의 값이 낮다 != 사람이 직접 느끼기에 좋은 언어 모델이다.
- PPL은 테스트 데이터에 의존 : 두 개 이상의 언어 모델을 비교할 때는 정량적으로 양이 많고, 도메인에 알맞은 동일한 테스트 데이터를 사용 → 신뢰도 높다.

# 3. 기존 언어 모델 VS 인공 신경망을 이용한 언어 모델

![image](https://user-images.githubusercontent.com/57162812/149070435-345b418c-d829-4460-a740-170f6e44f46a.png)

→ 대부분 n-gram을 이용한 언어 모델보다 좋은 성능 평가를 받았다.
