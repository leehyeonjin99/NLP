# 1. 손실 함수(Loss function)

![image](https://user-images.githubusercontent.com/57162812/149654768-530e3fe2-fa5a-47f5-a5a5-0bdd8c06a339.png)

- 손실 함수 : 실제값과 예측값의 차이를 수치화해주는 함수
   - 회귀 : 평균 제곱 오차
   - 분류 : 크로스 엔트로피
   
손실 함수의 값을 최소화하는 두 개의 매개변수인 가중치 w와 편향 b의 값을 찾는 것이 딥 러닝으 학습 과정이므로 손실 함수의 선정은 매우 중요

#### 1) MSE(Mean Squared Error)

연속형 변수 예측시 사용

```python
model.compile(optimizer='adam', loss='mse', metrics=['mse'])
```

```python
model.compile(optimizer='adam', loss=tf.keras.losses.MeanSquaredError(), metrics=['mse'])
```

#### 2) 이진 크로스 엔트로피(Binary Cross-Entropy)

출력층에서 시그모이드 함수를 사용하는 이진 분류시 사용

```python
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])
```

```python
model.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(), metrics=['acc'])
```

#### 3) 카테고리칼 크로스 엔트로피(Categorical Cross-Entropy)

출력층에서 소프트맥스 함수를 사용하는 다중 클래스 분류시 사용

```python
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])
```

```python
model.compile(optimizer='adam', loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['acc'])
```

원-핫 인코딩 과정을 생략하고, 정수값을 가진 레이블에 대해서 다중 클래스 분류를 수행시

```python
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])
```

```python
model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['acc'])
```

# 2. 배치 크기(Batch Size)에 따른 경사 하강법

![image](https://user-images.githubusercontent.com/57162812/149654990-d9310171-e1fe-4dd5-a053-ef1fc811a6a6.png)

- **배치(Batch)** : 가중치 등의 매개 변수의 값을 조정하기 위해 사용하는 데이터의 양

#### 1) 배치 경사 하강법(Batch Gradient Descent)

- 오차를 구할 때 전체 데이터 고려
- 한 번의 에포크(훈련)에 모든 매개변수 업데이트를 단 한 번 수행
- (단점) 한 번의 매개 변수 업데이트에 긴 시간, 메모리를 크게 요구

```python
model.fit(X_train, y_train, batch_size=len(X_train))
```

#### 2) 배치 크기가 1인 확률적 경사 하강법(Stochastic Gradient Descent. SGD)

- 매개변수 값 조정 시 전체 데이터가 아니라 랜덤으로 하나의 데이터에 대해서만 계산
- (장점) 빠르다.

![image](https://user-images.githubusercontent.com/57162812/149655175-61625435-27e9-45b2-a454-4b129810e5dd.png)

- (단점) 매개변수의 변경폭 불안정, 배치 경사 하강법보다 정확도가 낮을 수 있다.
- (장점) 하나의 데이터에 대해서만 메모리에 저장 → 자원이 적은 컴퓨터에서도 쉽게 사용 가능

```python
model.fit(X_train, y_train, batch_size=1)
```

#### 3) 미니 배치 경사 하강법(Mini-Batch Gradient Descent)

- (전제 데이터도, 1개의 데이터도 아닌)해당 데이터 개수만큼에 대해서 계산하여 매개 변수의 값 조정
- (장점) 속도 : 배치 경사 하강법보다 빠름
- (장점) 안정성 : SGD보다 안정적

```python
model.fit(X_train, y_train, batch_size-128)
```

- 배치 크기는 2의 n제곱에 해당하는 숫자로 선택하는 것이 보편적
- batch_size의 default 값 = 32

# 3. 옵티마이저(Optimizer)

#### 1) 모멘텀(Momentum)

- 관성의 법칙을 응용한 방법
- 경사 하강법에서 계산된 접선의 기울기에 한 시점 전의 접선의 기울기값을 일정한 비율만큼 반영
- (장점) **로컬 미니멈**에 도달했을 때, 관성의 힘을 빌려 값이 조절되면서 로컬 미니멈에서 탈출하고 **글로벌 미니멈** 내지는 더 낮은 로컬 미니멈으로 갈 수 있다.

```python
tensorflow.keras.optimizer.SGD(lr=0.01, momentum=0.9)
```

#### 2) 아다그라드(Adagrad)

- 각 매개변수에 서로 다른 학습률 적용
- 변화가 많은 매개변수는 학습률을 작게, 변화가 적은 학습률에는 높게 설정
- (단점) 학습을 계속 진행 시, 학습률이 지나치게 떨어진다.
 
```python
tensorflow.keras.optimizer.SGD(lr=0.01, epsilon=1e-06)
```

#### 3) 알엠에스프롭(RMSprop)

- Adagrad의 단점을 개선

```python
tensorflow.keras.optimizer.SGD(lr=0.001, rho=0.9, epsilon=1e-06)
```

#### 4) 아담(Adam)

- 모멘텀 + RMSprop
- 방향과 학습률 두 가지를 모두 잡기 위한 방법

```python
tensorflow.keras.optimizer.SGD(lr-0.001, beta_1=0.9, beta_2=0.999, epsilon=Nonem, decay=0.0, amsgrad=False)
```

#### 5) 사용 방법

```python
adam=tensorflow.keras.optimizer.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, ansgrad-False)
model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['acc'])
```

```python
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])
```

# 4. 에포크와 배치 크기와 이터레이션(Epochs and Batch size and Iteration)

![image](https://user-images.githubusercontent.com/57162812/149655836-ba007d0f-9bd9-40fa-9864-6e571dedf761.png)

#### 1) 에포크(Epoch)

- 인공 신경망에서 전체 데이터에 대해서 순전파와 역전파가 끝난 상태
- (비유) 문제지의 모든 문제를 끝까지 다 풀고, 정답지로 채점을 하여 문제지에 대한 공부를 한 번 끝낸 상태
- (예시) epoch=50 → 전체 단위로 총 50번 학습

#### 2) 배치 크기(Batch size)

- 몇 개의 데이터 단위로 매개 변수를 업데이트 하는지
- (비유) 문제지에서 몇 개씩 문제를 풀고나서 정답지를 확인하는지
- (예시) batch_size=200 → 200개의 샘플 단위로 가중치 업데이트
- 배치 크기 != 배치의 수 : 전체 데이터=2000, 배치 크기=200 → 배치의 수=10=이터레이션

#### 3) 이터레이션(Iteration) 또는 스텝(Step)

- 한 번의 에포크를 끝내기 위해서 필요한 배치의 수
- 한 번의 에포크 당 매개변수 업데이터가 이루어지는 수
