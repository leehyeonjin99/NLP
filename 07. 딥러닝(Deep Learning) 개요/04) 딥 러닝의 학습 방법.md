# 1. 손실 함수(Loss function)

![image](https://user-images.githubusercontent.com/57162812/149654768-530e3fe2-fa5a-47f5-a5a5-0bdd8c06a339.png)

- 손실 함수 : 실제값과 예측값의 차이를 수치화해주는 함수
   - 회귀 : 평균 제곱 오차
   - 분류 : 크로스 엔트로피
   
손실 함수의 값을 최소화하는 두 개의 매개변수인 가중치 w와 편향 b의 값을 찾는 것이 딥 러닝으 학습 과정이므로 손실 함수의 선정은 매우 중요

#### 1) MSE(Mean Squared Error)

연속형 변수 예측시 사용

```python
model.compile(optimizer='adam', loss='mse', metrics=['mse'])
```

```python
model.compile(optimizer='adam', loss=tf.keras.losses.MeanSquaredError(), metrics=['mse'])
```

#### 2) 이진 크로스 엔트로피(Binary Cross-Entropy)

출력층에서 시그모이드 함수를 사용하는 이진 분류시 사용

```python
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])
```

```python
model.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(), metrics=['acc'])
```

#### 3) 카테고리칼 크로스 엔트로피(Categorical Cross-Entropy)

출력층에서 소프트맥스 함수를 사용하는 다중 클래스 분류시 사용

```python
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])
```

```python
model.compile(optimizer='adam', loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['acc'])
```

원-핫 인코딩 과정을 생략하고, 정수값을 가진 레이블에 대해서 다중 클래스 분류를 수행시

```python
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])
```

```python
model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['acc'])
```

# 2. 배치 크기(Batch Size)에 따른 경사 하강법

![image](https://user-images.githubusercontent.com/57162812/149654990-d9310171-e1fe-4dd5-a053-ef1fc811a6a6.png)

- **배치(Batch)** : 가중치 등의 매개 변수의 값을 조정하기 위해 사용하는 데이터의 양

#### 1) 배치 경사 하강법(Batch Gradient Descent)

- 오차를 구할 때 전체 데이터 고려
- 한 번의 에포크(훈련)에 모든 매개변수 업데이트를 단 한 번 수행
- (단점) 한 번의 매개 변수 업데이트에 긴 시간, 메모리를 크게 요구

```python
model.fit(X_train, y_train, batch_size=len(X_train))
```

#### 2) 배치 크기가 1인 확률적 경사 하강법(Stochastic Gradient Descent. SGD)

- 매개변수 값 조정 시 전체 데이터가 아니라 랜덤으로 하나의 데이터에 대해서만 계산
- (장점) 빠르다.

![image](https://user-images.githubusercontent.com/57162812/149655175-61625435-27e9-45b2-a454-4b129810e5dd.png)

- (단점) 매개변수의 변경폭 불안정, 배치 경사 하강법보다 정확도가 낮을 수 있다.
- (장점) 하나의 데이터에 대해서만 메모리에 저장 → 자원이 적은 컴퓨터에서도 쉽게 사용 가능

```python
model.fit(X_train, y_train, batch_size=1)
```

#### 3) 미니 배치 경사 하강법(Mini-Batch Gradient Descent)

- (전제 데이터도, 1개의 데이터도 아닌)해당 데이터 개수만큼에 대해서 계산하여 매개 변수의 값 조정
- (장점) 속도 : 배치 경사 하강법보다 빠름
- (장점) 안정성 : SGD보다 안정적

```python
model.fit(X_train, y_train, batch_size-128)
```

- 배치 크기는 2의 n제곱에 해당하는 숫자로 선택하는 것이 보편적
- batch_size의 default 값 = 32

# 3. 옵티마이저(Optimizer)

#### 1) 모멘텀(Momentum)

- 관성의 법칙을 응용한 방법
- 경사 하강법에서 계산된 접선의 기울기에 한 시점 전의 접선의 기울기값을 일정한 비율만큼 반영
- (장점) **로컬 미니멈**에 도달했을 때, 관성의 힘을 빌려 값이 조절되면서 로컬 미니멈에서 탈출하고 **글로벌 미니멈** 내지는 더 낮은 로컬 미니멈으로 갈 수 있다.

```python
tensorflow.keras.optimizer.SGD(lr=0.01, momentum=0.9)
```

#### 2) 아다그라드(Adagrad)

- 각 매개변수에 서로 다른 학습률 적용
- 변화가 많은 매개변수는 학습률을 작게, 변화가 적은 학습률에는 높게 설정
- (단점) 학습을 계속 진행 시, 학습률이 지나치게 떨어진다.
 
```python
tensorflow.keras.optimizer.SGD(lr=0.01, epsilon=1e-06)
```

#### 3) 알엠에스프롭(RMSprop)

- Adagrad의 단점을 개선

```python
tensorflow.keras.optimizer.SGD(lr=0.001, rho=0.9, epsilon=1e-06)
```

#### 4) 아담(Adam)

- 모멘텀 + RMSprop
- 방향과 학습률 두 가지를 모두 잡기 위한 방법

```python
tensorflow.keras.optimizer.SGD(lr-0.001, beta_1=0.9, beta_2=0.999, epsilon=Nonem, decay=0.0, amsgrad=False)
```

#### 5) 사용 방법

```python
adam=tensorflow.keras.optimizer.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, ansgrad-False)
model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['acc'])
```

```python
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])
```

# 4. 역전파(BackPropagation)

#### 1) 인공 신경망의 이해

- 인공 신경망 = 입력층 + 은닉층 + 출력층

![image](https://user-images.githubusercontent.com/57162812/149657241-d782811a-a037-4cc4-b2af-9b8fb4a0d0d0.png)

두개의 입력, 두 개의 은닉층 뉴런, 두 개의 출력층 뉴런, 은닉층과 출력층의 모든 뉴런은 활성화 함수로 시그모이드 함수 사용

#### 2) 순전파(Forward Propagation)

![image](https://user-images.githubusercontent.com/57162812/149657455-579c453c-4b20-40e0-86f0-78d3c9c579d8.png)

- z1=0.03+0.05=0.08
- z2=0.04+0.07=0.11  



- h1=sigmoid(z1)=0.51998934
- h2=sigmoid(z2)=0.52747230  



- z3=0.45h1+0.4h2=0.44498412
- z4=0.7h1+0.6h2=0.68047592  



- o1=sigmoid(z3)=0.60944600
- o2=sigmoid(z4)=0.66384491  

오차 함수 : MSE

- E1=1/2(target1-o1)^2=0.02193381
- E2=1/2(target1-o2)^2=0.00203809
- E=E1+E2=0.02397190

#### 3) 역전파 1단계

![image](https://user-images.githubusercontent.com/57162812/149658295-f926b84c-783f-4a3c-aef3-a6c0eb26ab80.png)

- 업데이트 해야할 가중치 : w5, w6, w7, w8

![image](https://user-images.githubusercontent.com/57162812/149658458-8f2ad23f-0184-40d4-9818-602344867974.png)
![image](https://user-images.githubusercontent.com/57162812/149658470-5753eb8e-3d49-48f6-ae66-8a68ef2a9931.png)
![image](https://user-images.githubusercontent.com/57162812/149658482-6b8d9919-f1e9-4941-9dcc-6b67a4d71a8b.png)
![image](https://user-images.githubusercontent.com/57162812/149658513-c06d3f41-8ea7-44eb-8d1d-56138d4e55be.png)

시그모이드 함수의 미분 : f(x)x(1-f(x))

![image](https://user-images.githubusercontent.com/57162812/149658619-66d532a7-b358-403b-80f9-e59469504934.png)

![image](https://user-images.githubusercontent.com/57162812/149658633-7624eb07-16de-4221-806c-6930eb39ab66.png)

![image](https://user-images.githubusercontent.com/57162812/149658643-82a30c4b-a960-4253-801e-a53496d50c06.png)

(가정) 학습률=0.5

![image](https://user-images.githubusercontent.com/57162812/149658662-3faf7fee-6a0a-4b83-8b4b-bc1fae19b4ed.png)

#### 4) 역전파 2단계

![image](https://user-images.githubusercontent.com/57162812/149658787-78c9b818-1adf-490f-a7f3-08471fcf7d42.png)

![image](https://user-images.githubusercontent.com/57162812/149658799-c1614bb3-3afe-4b7b-b8fc-cf4c9f31ddf9.png)

![image](https://user-images.githubusercontent.com/57162812/149658808-d555abbe-5541-4edd-8001-7ba930cd7c7f.png)

#### 5) 결과 확인

![image](https://user-images.githubusercontent.com/57162812/149658833-a675ee60-c106-4cf1-bd67-21909ee8fb8f.png)

전체 오차가 1번의 역전파로 감소한 것을 확인할 수 있다.  
인공 신경망의 학습은 오차를 최소화하는 가중치를 찾는 목적으로 순전파와 역전파를 반복하는 것을 뜻한다.

# 5. 에포크와 배치 크기와 이터레이션(Epochs and Batch size and Iteration)

![image](https://user-images.githubusercontent.com/57162812/149655836-ba007d0f-9bd9-40fa-9864-6e571dedf761.png)

#### 1) 에포크(Epoch)

- 인공 신경망에서 전체 데이터에 대해서 순전파와 역전파가 끝난 상태
- (비유) 문제지의 모든 문제를 끝까지 다 풀고, 정답지로 채점을 하여 문제지에 대한 공부를 한 번 끝낸 상태
- (예시) epoch=50 → 전체 단위로 총 50번 학습

#### 2) 배치 크기(Batch size)

- 몇 개의 데이터 단위로 매개 변수를 업데이트 하는지
- (비유) 문제지에서 몇 개씩 문제를 풀고나서 정답지를 확인하는지
- (예시) batch_size=200 → 200개의 샘플 단위로 가중치 업데이트
- 배치 크기 != 배치의 수 : 전체 데이터=2000, 배치 크기=200 → 배치의 수=10=이터레이션

#### 3) 이터레이션(Iteration) 또는 스텝(Step)

- 한 번의 에포크를 끝내기 위해서 필요한 배치의 수
- 한 번의 에포크 당 매개변수 업데이터가 이루어지는 수
